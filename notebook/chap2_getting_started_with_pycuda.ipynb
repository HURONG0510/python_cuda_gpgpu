{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying your GPU with PyCUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们要实现使用pycuda进行deviceQuery的设计\n",
    "\n",
    "- intializing CUDA as follows:\n",
    "```python\n",
    "import pycuda.driver as drv\n",
    "drv.init()\n",
    "```\n",
    "或者\n",
    "```python\n",
    "import pycuda.autoinit\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 2 CUDA Capable device(s)\n",
      "Device 0: NVIDIA TITAN RTX\n",
      "Capability  (7, 5)\n",
      "Memory   24205 MB\n",
      "the number of streaming multiprocessor: 72\n",
      "Device 1: NVIDIA TITAN RTX\n",
      "Capability  (7, 5)\n",
      "Memory   24205 MB\n",
      "the number of streaming multiprocessor: 72\n"
     ]
    }
   ],
   "source": [
    "import pycuda.driver as drv \n",
    "drv.init()\n",
    "print(f'Detected {drv.Device.count()} CUDA Capable device(s)')\n",
    "\n",
    "for i in range(drv.Device.count()):\n",
    "    gpu_device = drv.Device(i)\n",
    "    print ('Device {}: {}'.format(i, gpu_device.name()))\n",
    "    compute_capability = gpu_device.compute_capability()\n",
    "    print(f'Capability  {compute_capability}')\n",
    "    print(f'Memory   {gpu_device.total_memory()//(1024**2)} MB')\n",
    "\n",
    "    # gpu_device.get_attributes()是一个字典，但是他的key是Enumeration Type，不是字符串\n",
    "    device_attributes = {}\n",
    "    for k,v in gpu_device.get_attributes().items():\n",
    "        device_attributes[str(k)] = v\n",
    "    \n",
    "\n",
    "\n",
    "    # for k,v in device_attributes.items():\n",
    "    #     print(f'{k}: {v}')\n",
    "    # print(device_attributes)\n",
    "\n",
    "    # the number of multiprocessor\n",
    "    num_mp = device_attributes['MULTIPROCESSOR_COUNT']\n",
    "    print(f'the number of streaming multiprocessor: {num_mp}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A GPU divides its individual cores up into lager units known as Streaming Multiprocessor(SM), which will each individuallyhave a particular number of CUDA cores, depending on the compute capability.\n",
    "\n",
    "The number of cores per sm is not indicated directly by the gpu --- this is given to us implicitly by the compute capability. We will have to look up some techinical documents to determine the number of cores per sm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 2 CUDA Capable device(s)\n",
      "Device 0: NVIDIA TITAN RTX\n",
      "Capability  7.5\n",
      "Memory   24205 MB\n",
      "the number of streaming multiprocessor: 72\n",
      "the total number of cuda cores: 4608\n",
      "ASYNC_ENGINE_COUNT : 3\n",
      "CAN_MAP_HOST_MEMORY : 1\n",
      "CAN_USE_HOST_POINTER_FOR_REGISTERED_MEM : 1\n",
      "CLOCK_RATE : 1770000\n",
      "COMPUTE_CAPABILITY_MAJOR : 7\n",
      "COMPUTE_CAPABILITY_MINOR : 5\n",
      "COMPUTE_MODE : DEFAULT\n",
      "COMPUTE_PREEMPTION_SUPPORTED : 1\n",
      "CONCURRENT_KERNELS : 1\n",
      "CONCURRENT_MANAGED_ACCESS : 1\n",
      "DIRECT_MANAGED_MEM_ACCESS_FROM_HOST : 0\n",
      "ECC_ENABLED : 0\n",
      "GENERIC_COMPRESSION_SUPPORTED : 0\n",
      "GLOBAL_L1_CACHE_SUPPORTED : 1\n",
      "GLOBAL_MEMORY_BUS_WIDTH : 384\n",
      "GPU_OVERLAP : 1\n",
      "HANDLE_TYPE_POSIX_FILE_DESCRIPTOR_SUPPORTED : 1\n",
      "HANDLE_TYPE_WIN32_HANDLE_SUPPORTED : 0\n",
      "HANDLE_TYPE_WIN32_KMT_HANDLE_SUPPORTED : 0\n",
      "HOST_NATIVE_ATOMIC_SUPPORTED : 0\n",
      "INTEGRATED : 0\n",
      "KERNEL_EXEC_TIMEOUT : 1\n",
      "L2_CACHE_SIZE : 6291456\n",
      "LOCAL_L1_CACHE_SUPPORTED : 1\n",
      "MANAGED_MEMORY : 1\n",
      "MAXIMUM_SURFACE1D_LAYERED_LAYERS : 2048\n",
      "MAXIMUM_SURFACE1D_LAYERED_WIDTH : 32768\n",
      "MAXIMUM_SURFACE1D_WIDTH : 32768\n",
      "MAXIMUM_SURFACE2D_HEIGHT : 65536\n",
      "MAXIMUM_SURFACE2D_LAYERED_HEIGHT : 32768\n",
      "MAXIMUM_SURFACE2D_LAYERED_LAYERS : 2048\n",
      "MAXIMUM_SURFACE2D_LAYERED_WIDTH : 32768\n",
      "MAXIMUM_SURFACE2D_WIDTH : 131072\n",
      "MAXIMUM_SURFACE3D_DEPTH : 16384\n",
      "MAXIMUM_SURFACE3D_HEIGHT : 16384\n",
      "MAXIMUM_SURFACE3D_WIDTH : 16384\n",
      "MAXIMUM_SURFACECUBEMAP_LAYERED_LAYERS : 2046\n",
      "MAXIMUM_SURFACECUBEMAP_LAYERED_WIDTH : 32768\n",
      "MAXIMUM_SURFACECUBEMAP_WIDTH : 32768\n",
      "MAXIMUM_TEXTURE1D_LAYERED_LAYERS : 2048\n",
      "MAXIMUM_TEXTURE1D_LAYERED_WIDTH : 32768\n",
      "MAXIMUM_TEXTURE1D_LINEAR_WIDTH : 268435456\n",
      "MAXIMUM_TEXTURE1D_MIPMAPPED_WIDTH : 32768\n",
      "MAXIMUM_TEXTURE1D_WIDTH : 131072\n",
      "MAXIMUM_TEXTURE2D_ARRAY_HEIGHT : 32768\n",
      "MAXIMUM_TEXTURE2D_ARRAY_NUMSLICES : 2048\n",
      "MAXIMUM_TEXTURE2D_ARRAY_WIDTH : 32768\n",
      "MAXIMUM_TEXTURE2D_GATHER_HEIGHT : 32768\n",
      "MAXIMUM_TEXTURE2D_GATHER_WIDTH : 32768\n",
      "MAXIMUM_TEXTURE2D_HEIGHT : 65536\n",
      "MAXIMUM_TEXTURE2D_LINEAR_HEIGHT : 65000\n",
      "MAXIMUM_TEXTURE2D_LINEAR_PITCH : 2097120\n",
      "MAXIMUM_TEXTURE2D_LINEAR_WIDTH : 131072\n",
      "MAXIMUM_TEXTURE2D_MIPMAPPED_HEIGHT : 32768\n",
      "MAXIMUM_TEXTURE2D_MIPMAPPED_WIDTH : 32768\n",
      "MAXIMUM_TEXTURE2D_WIDTH : 131072\n",
      "MAXIMUM_TEXTURE3D_DEPTH : 16384\n",
      "MAXIMUM_TEXTURE3D_DEPTH_ALTERNATE : 32768\n",
      "MAXIMUM_TEXTURE3D_HEIGHT : 16384\n",
      "MAXIMUM_TEXTURE3D_HEIGHT_ALTERNATE : 8192\n",
      "MAXIMUM_TEXTURE3D_WIDTH : 16384\n",
      "MAXIMUM_TEXTURE3D_WIDTH_ALTERNATE : 8192\n",
      "MAXIMUM_TEXTURECUBEMAP_LAYERED_LAYERS : 2046\n",
      "MAXIMUM_TEXTURECUBEMAP_LAYERED_WIDTH : 32768\n",
      "MAXIMUM_TEXTURECUBEMAP_WIDTH : 32768\n",
      "MAX_BLOCKS_PER_MULTIPROCESSOR : 16\n",
      "MAX_BLOCK_DIM_X : 1024\n",
      "MAX_BLOCK_DIM_Y : 1024\n",
      "MAX_BLOCK_DIM_Z : 64\n",
      "MAX_GRID_DIM_X : 2147483647\n",
      "MAX_GRID_DIM_Y : 65535\n",
      "MAX_GRID_DIM_Z : 65535\n",
      "MAX_PERSISTING_L2_CACHE_SIZE : 0\n",
      "MAX_PITCH : 2147483647\n",
      "MAX_REGISTERS_PER_BLOCK : 65536\n",
      "MAX_REGISTERS_PER_MULTIPROCESSOR : 65536\n",
      "MAX_SHARED_MEMORY_PER_BLOCK : 49152\n",
      "MAX_SHARED_MEMORY_PER_BLOCK_OPTIN : 65536\n",
      "MAX_SHARED_MEMORY_PER_MULTIPROCESSOR : 65536\n",
      "MAX_THREADS_PER_BLOCK : 1024\n",
      "MAX_THREADS_PER_MULTIPROCESSOR : 1024\n",
      "MEMORY_CLOCK_RATE : 7001000\n",
      "MEMORY_POOLS_SUPPORTED : 1\n",
      "MULTIPROCESSOR_COUNT : 72\n",
      "MULTI_GPU_BOARD : 0\n",
      "MULTI_GPU_BOARD_GROUP_ID : 0\n",
      "PAGEABLE_MEMORY_ACCESS : 0\n",
      "PAGEABLE_MEMORY_ACCESS_USES_HOST_PAGE_TABLES : 0\n",
      "PCI_BUS_ID : 59\n",
      "PCI_DEVICE_ID : 0\n",
      "PCI_DOMAIN_ID : 0\n",
      "READ_ONLY_HOST_REGISTER_SUPPORTED : 1\n",
      "RESERVED_SHARED_MEMORY_PER_BLOCK : 0\n",
      "SINGLE_TO_DOUBLE_PRECISION_PERF_RATIO : 32\n",
      "STREAM_PRIORITIES_SUPPORTED : 1\n",
      "SURFACE_ALIGNMENT : 512\n",
      "TCC_DRIVER : 0\n",
      "TEXTURE_ALIGNMENT : 512\n",
      "TEXTURE_PITCH_ALIGNMENT : 32\n",
      "TOTAL_CONSTANT_MEMORY : 65536\n",
      "UNIFIED_ADDRESSING : 1\n",
      "WARP_SIZE : 32\n",
      "Device 1: NVIDIA TITAN RTX\n",
      "Capability  7.5\n",
      "Memory   24205 MB\n",
      "the number of streaming multiprocessor: 72\n",
      "the total number of cuda cores: 4608\n",
      "ASYNC_ENGINE_COUNT : 3\n",
      "CAN_MAP_HOST_MEMORY : 1\n",
      "CAN_USE_HOST_POINTER_FOR_REGISTERED_MEM : 1\n",
      "CLOCK_RATE : 1770000\n",
      "COMPUTE_CAPABILITY_MAJOR : 7\n",
      "COMPUTE_CAPABILITY_MINOR : 5\n",
      "COMPUTE_MODE : DEFAULT\n",
      "COMPUTE_PREEMPTION_SUPPORTED : 1\n",
      "CONCURRENT_KERNELS : 1\n",
      "CONCURRENT_MANAGED_ACCESS : 1\n",
      "DIRECT_MANAGED_MEM_ACCESS_FROM_HOST : 0\n",
      "ECC_ENABLED : 0\n",
      "GENERIC_COMPRESSION_SUPPORTED : 0\n",
      "GLOBAL_L1_CACHE_SUPPORTED : 1\n",
      "GLOBAL_MEMORY_BUS_WIDTH : 384\n",
      "GPU_OVERLAP : 1\n",
      "HANDLE_TYPE_POSIX_FILE_DESCRIPTOR_SUPPORTED : 1\n",
      "HANDLE_TYPE_WIN32_HANDLE_SUPPORTED : 0\n",
      "HANDLE_TYPE_WIN32_KMT_HANDLE_SUPPORTED : 0\n",
      "HOST_NATIVE_ATOMIC_SUPPORTED : 0\n",
      "INTEGRATED : 0\n",
      "KERNEL_EXEC_TIMEOUT : 1\n",
      "L2_CACHE_SIZE : 6291456\n",
      "LOCAL_L1_CACHE_SUPPORTED : 1\n",
      "MANAGED_MEMORY : 1\n",
      "MAXIMUM_SURFACE1D_LAYERED_LAYERS : 2048\n",
      "MAXIMUM_SURFACE1D_LAYERED_WIDTH : 32768\n",
      "MAXIMUM_SURFACE1D_WIDTH : 32768\n",
      "MAXIMUM_SURFACE2D_HEIGHT : 65536\n",
      "MAXIMUM_SURFACE2D_LAYERED_HEIGHT : 32768\n",
      "MAXIMUM_SURFACE2D_LAYERED_LAYERS : 2048\n",
      "MAXIMUM_SURFACE2D_LAYERED_WIDTH : 32768\n",
      "MAXIMUM_SURFACE2D_WIDTH : 131072\n",
      "MAXIMUM_SURFACE3D_DEPTH : 16384\n",
      "MAXIMUM_SURFACE3D_HEIGHT : 16384\n",
      "MAXIMUM_SURFACE3D_WIDTH : 16384\n",
      "MAXIMUM_SURFACECUBEMAP_LAYERED_LAYERS : 2046\n",
      "MAXIMUM_SURFACECUBEMAP_LAYERED_WIDTH : 32768\n",
      "MAXIMUM_SURFACECUBEMAP_WIDTH : 32768\n",
      "MAXIMUM_TEXTURE1D_LAYERED_LAYERS : 2048\n",
      "MAXIMUM_TEXTURE1D_LAYERED_WIDTH : 32768\n",
      "MAXIMUM_TEXTURE1D_LINEAR_WIDTH : 268435456\n",
      "MAXIMUM_TEXTURE1D_MIPMAPPED_WIDTH : 32768\n",
      "MAXIMUM_TEXTURE1D_WIDTH : 131072\n",
      "MAXIMUM_TEXTURE2D_ARRAY_HEIGHT : 32768\n",
      "MAXIMUM_TEXTURE2D_ARRAY_NUMSLICES : 2048\n",
      "MAXIMUM_TEXTURE2D_ARRAY_WIDTH : 32768\n",
      "MAXIMUM_TEXTURE2D_GATHER_HEIGHT : 32768\n",
      "MAXIMUM_TEXTURE2D_GATHER_WIDTH : 32768\n",
      "MAXIMUM_TEXTURE2D_HEIGHT : 65536\n",
      "MAXIMUM_TEXTURE2D_LINEAR_HEIGHT : 65000\n",
      "MAXIMUM_TEXTURE2D_LINEAR_PITCH : 2097120\n",
      "MAXIMUM_TEXTURE2D_LINEAR_WIDTH : 131072\n",
      "MAXIMUM_TEXTURE2D_MIPMAPPED_HEIGHT : 32768\n",
      "MAXIMUM_TEXTURE2D_MIPMAPPED_WIDTH : 32768\n",
      "MAXIMUM_TEXTURE2D_WIDTH : 131072\n",
      "MAXIMUM_TEXTURE3D_DEPTH : 16384\n",
      "MAXIMUM_TEXTURE3D_DEPTH_ALTERNATE : 32768\n",
      "MAXIMUM_TEXTURE3D_HEIGHT : 16384\n",
      "MAXIMUM_TEXTURE3D_HEIGHT_ALTERNATE : 8192\n",
      "MAXIMUM_TEXTURE3D_WIDTH : 16384\n",
      "MAXIMUM_TEXTURE3D_WIDTH_ALTERNATE : 8192\n",
      "MAXIMUM_TEXTURECUBEMAP_LAYERED_LAYERS : 2046\n",
      "MAXIMUM_TEXTURECUBEMAP_LAYERED_WIDTH : 32768\n",
      "MAXIMUM_TEXTURECUBEMAP_WIDTH : 32768\n",
      "MAX_BLOCKS_PER_MULTIPROCESSOR : 16\n",
      "MAX_BLOCK_DIM_X : 1024\n",
      "MAX_BLOCK_DIM_Y : 1024\n",
      "MAX_BLOCK_DIM_Z : 64\n",
      "MAX_GRID_DIM_X : 2147483647\n",
      "MAX_GRID_DIM_Y : 65535\n",
      "MAX_GRID_DIM_Z : 65535\n",
      "MAX_PERSISTING_L2_CACHE_SIZE : 0\n",
      "MAX_PITCH : 2147483647\n",
      "MAX_REGISTERS_PER_BLOCK : 65536\n",
      "MAX_REGISTERS_PER_MULTIPROCESSOR : 65536\n",
      "MAX_SHARED_MEMORY_PER_BLOCK : 49152\n",
      "MAX_SHARED_MEMORY_PER_BLOCK_OPTIN : 65536\n",
      "MAX_SHARED_MEMORY_PER_MULTIPROCESSOR : 65536\n",
      "MAX_THREADS_PER_BLOCK : 1024\n",
      "MAX_THREADS_PER_MULTIPROCESSOR : 1024\n",
      "MEMORY_CLOCK_RATE : 7001000\n",
      "MEMORY_POOLS_SUPPORTED : 1\n",
      "MULTIPROCESSOR_COUNT : 72\n",
      "MULTI_GPU_BOARD : 0\n",
      "MULTI_GPU_BOARD_GROUP_ID : 1\n",
      "PAGEABLE_MEMORY_ACCESS : 0\n",
      "PAGEABLE_MEMORY_ACCESS_USES_HOST_PAGE_TABLES : 0\n",
      "PCI_BUS_ID : 175\n",
      "PCI_DEVICE_ID : 0\n",
      "PCI_DOMAIN_ID : 0\n",
      "READ_ONLY_HOST_REGISTER_SUPPORTED : 1\n",
      "RESERVED_SHARED_MEMORY_PER_BLOCK : 0\n",
      "SINGLE_TO_DOUBLE_PRECISION_PERF_RATIO : 32\n",
      "STREAM_PRIORITIES_SUPPORTED : 1\n",
      "SURFACE_ALIGNMENT : 512\n",
      "TCC_DRIVER : 0\n",
      "TEXTURE_ALIGNMENT : 512\n",
      "TEXTURE_PITCH_ALIGNMENT : 32\n",
      "TOTAL_CONSTANT_MEMORY : 65536\n",
      "UNIFIED_ADDRESSING : 1\n",
      "WARP_SIZE : 32\n"
     ]
    }
   ],
   "source": [
    "import pycuda.driver as drv \n",
    "drv.init()\n",
    "print(f'Detected {drv.Device.count()} CUDA Capable device(s)')\n",
    "\n",
    "for i in range(drv.Device.count()):\n",
    "    gpu_device = drv.Device(i)\n",
    "    print ('Device {}: {}'.format(i, gpu_device.name()))\n",
    "    compute_capability = gpu_device.compute_capability()\n",
    "    compute_capability = f'{compute_capability[0]}.{compute_capability[1]}'\n",
    "    print(f'Capability  {compute_capability}')\n",
    "    print(f'Memory   {gpu_device.total_memory()//(1024**2)} MB')\n",
    "\n",
    "    # gpu_device.get_attributes()是一个字典，但是他的key是Enumeration Type，不是字符串\n",
    "    device_attributes = {}\n",
    "    for k,v in gpu_device.get_attributes().items():\n",
    "        device_attributes[str(k)] = v\n",
    "    \n",
    "    # the number of multiprocessor\n",
    "    num_sm = device_attributes['MULTIPROCESSOR_COUNT']\n",
    "    print(f'the number of streaming multiprocessor: {num_mp}')\n",
    "\n",
    "    # 根据计算能力推断每个 SM 的核心数\n",
    "    cores_per_sm = {\n",
    "        '3.0': 192,  # Kepler\n",
    "        '3.5': 192,\n",
    "        '5.0': 128,  # Maxwell\n",
    "        '5.2': 128,\n",
    "        '6.0': 64,   # Pascal\n",
    "        '6.1': 128,\n",
    "        '7.0': 64,   # Volta\n",
    "        '7.5': 64,   # Turing\n",
    "        '8.0': 128,  # Ampere\n",
    "        '8.6': 128\n",
    "    }[compute_capability]\n",
    "\n",
    "    # the number of cuda cores on device\n",
    "    print(f'the total number of cuda cores: {cores_per_sm*num_sm}')\n",
    "\n",
    "    for k,v in device_attributes.items():\n",
    "        print(f'{k} : {v}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using pycuda' gpuarray class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like numpy, pycuda's gpuarray class plays an analogously prominent role within GPU programming in Python.\n",
    "\n",
    "This class has all the features like numpy --- multidimentional vector/matrix/tensor shape structuring, array-slicing, array unraveling and overload operators for point-wise computations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transferring data to and from the gpu with gpuarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pycuda covers all of the overhead of memory allocation, deallocation and data transfers with the `gpuarray` class.\n",
    "\n",
    "与`numpy`类似，使用vector/matrix/tensor shape structure for data. `gpuarray` objects perform automatic cleanup based on the lifetime, so we do not have to worry about **freeing any GPU memory** stored in a `gpuarray` object when we are done with it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. contain host data in some form of numpy array\n",
    "2. use the `gpuarray.to_gpu(host_data)` command to transfer this over to the gpu and create a new gpu array\n",
    "3. after computatio within gpu, retrieve the gpu data into new host memory with the  `gpuarray.get` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.  4.  6.  8. 10.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pycuda.autoinit\n",
    "from pycuda import gpuarray\n",
    "host_data = np.array([1,2,3,4,5], dtype=np.float32)\n",
    "\n",
    "device_data = gpuarray.to_gpu(host_data)\n",
    "\n",
    "device_data_x2 = device_data * 2\n",
    "host_data_x2 = device_data_x2.get()\n",
    "\n",
    "print(host_data_x2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要显式注明numpy data type，因为 float type 与C/C++直接相关，且之后需要写 inline CUDA C。CUDA不使用double类型。\n",
    "\n",
    "```python\n",
    "host_data = np.array([1,2,3,4,5], dtype=np.float32)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic pointwise arithmetic operations with gpuarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the overloaded python multiplication operator (*) to multiple each element in a `gpuarray` object by a scalr value. \n",
    "\n",
    "A pointwise operation is intrinsically parallelizable, and so when we use this operation on a gpuarray object pycuda is able to offload each multiplication opeation onto a single thread, rather than computing each multiplication in serial (numpy can use the advanced SSE instructions found in modern x86 chips for these computations, so in some cases the performance will be comparable to a gpu).\n",
    "\n",
    "To be clear, **these pointwise operations performed on the GPU are in parallel** since the computation of one element is not dependent on the computation of any other element.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 3. 4.]\n",
      "<class 'pycuda.gpuarray.GPUArray'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pycuda.autoinit\n",
    "from pycuda import gpuarray\n",
    "x_host = np.array([1,2,3], dtype=np.float32)\n",
    "y_host = np.array([1,1,1], dtype=np.float32)\n",
    "z_host = np.array([2,2,2], dtype=np.float32)\n",
    "\n",
    "x_device = gpuarray.to_gpu(x_host)\n",
    "y_device = gpuarray.to_gpu(y_host)\n",
    "z_device = gpuarray.to_gpu(z_host)\n",
    "\n",
    "print(x_host + y_host)\n",
    "\n",
    "print((x_device + y_device).get())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A speed test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time to compute on CPU：0.17195940017700195 s\n",
      "total time to compute on GPU：0.0046041011810302734 s\n",
      "Is the host computation same as the Device computation?:True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pycuda.autoinit\n",
    "from pycuda import gpuarray\n",
    "from time import time\n",
    "\n",
    "# * np.random refers to the random module in Numpy, and the second random is the specific function generating an array of random floating-point numbers uniformly distributed in the range [0,1)\n",
    "host_data = np.array(np.random.random(5* 10**7), dtype=np.float32)\n",
    "\n",
    "t1 = time()\n",
    "host_data_2x = host_data * np.float32(2)\n",
    "t2 = time()\n",
    "\n",
    "print(f'total time to compute on CPU：{t2-t1} s')\n",
    "\n",
    "\n",
    "device_data = gpuarray.to_gpu(host_data)\n",
    "\n",
    "\n",
    "t1 = time()\n",
    "device_data_2x = device_data * np.float32(2)\n",
    "t2 = time()\n",
    "\n",
    "from_device = device_data_2x.get()\n",
    "print(f'total time to compute on GPU：{t2-t1} s')\n",
    "\n",
    "\n",
    "print(f'Is the host computation same as the Device computation?:{np.allclose(from_device, host_data_2x)}')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we execute these code twice, we first notice that the CPU computation time is about the same for each computation. Yet, we notice that the GPU computation time is far slower than CPU computation the first time we run this, and it becomes much faster in the subsequent run \n",
    "```bash\n",
    "total time to compute on CPU：0.16499710083007812 s\n",
    "total time to compute on GPU：0.28182029724121094 s\n",
    "Is the host computation same as the Device computation?:True\n",
    "```\n",
    "\n",
    "```bash\n",
    "total time to compute on CPU：0.16763687133789062 s\n",
    "total time to compute on GPU：0.0035741329193115234 s\n",
    "Is the host computation same as the Device computation?:True\n",
    "```\n",
    "\n",
    "Why is this? By the nature of the PyCUDA library, GPU code is often compiled and linked with Nvidia's `nvcc` compiler the first time it is run in a given python session; it is then cached and, if the code is called again, then it doesn't have to be recompiled.\n",
    "\n",
    "\n",
    "*In PyCUDA, GPU code is often compiled at runtime with the NVIDIA nvcc compiler and then subsequently called from PyCUDA. This can lead to an unexpected slowdown, usually the first time a program or GPU operation is run in a given Python session.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using PyCUDA's elementWisekernel for performing pointwise computations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll have to write a little bit of `inline code` in CUDA C, which is compiled externally by NVIDIA's `nvcc` compiler and then launched at runtime by our code via PyCUDA.\n",
    "\n",
    "We use the term **kernel**. By kernel, we always mean a function that is launched directly onto the GPU by CUDA. We will use several functions from PyCUDA that **generate templates and design patterns** for different types of kernels, easing our transition into GPU programming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time to compute on CPU: 0.2131824493408203 s\n",
      "total time to compute on GPU 0.2850375175476074 s\n",
      "Is the host computation same as the Device computation?:True\n",
      "total time to compute on CPU: 0.17487001419067383 s\n",
      "total time to compute on GPU 0.00023365020751953125 s\n",
      "Is the host computation same as the Device computation?:True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pycuda.autoinit\n",
    "from pycuda import gpuarray\n",
    "from time import time\n",
    "from pycuda.elementwise import ElementwiseKernel\n",
    "\n",
    "host_data = np.array(np.random.random(5 * 10**7), dtype=np.float32)\n",
    "'''\n",
    "first line: C pointers to allocate memory on GPU\n",
    "second line: define element-wise operation\n",
    "third line: Note PyCUDA automatically sets up the integer index i. Whenwe use i as our index, ElementwiseKernel will automatically parallelize our calcularion over i among the many cores\n",
    "forth line: give our piece of code its internal CUDA C kernel name. Since this refers to CUDA'C namespace and not Python's\n",
    "\n",
    "'''\n",
    "gpu_2x_ker = ElementwiseKernel(\n",
    "    \"float *in, float *out\",\n",
    "    \"out[i] = 2*in[i];\",\n",
    "    \"gpu_2x_ker\"\n",
    ")\n",
    "\n",
    "def speedcomparison():\n",
    "    t1 = time()\n",
    "    host_data_2x = host_data * np.float32(2)\n",
    "    t2 = time()\n",
    "\n",
    "    print(f\"total time to compute on CPU: {t2-t1} s\")\n",
    "\n",
    "    device_data = gpuarray.to_gpu(host_data)\n",
    "    # allocate memory for output\n",
    "    device_data_2x = gpuarray.empty_like(device_data)\n",
    "\n",
    "    t1 = time()\n",
    "    gpu_2x_ker(device_data, device_data_2x)\n",
    "    t2 = time()\n",
    "\n",
    "    from_device = device_data_2x.get()\n",
    "    print(f\"total time to compute on GPU {t2-t1} s\")\n",
    "\n",
    "    print(f'Is the host computation same as the Device computation?:{np.allclose(from_device, host_data_2x)}')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    speedcomparison()\n",
    "    speedcomparison()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce the time for recompiling and utilize the cache at runtime, we execute `speedcomparison` twice.\n",
    "\n",
    "When using PyCUDA, the first time we run a GPU kernel function that has CUDA C code written inside our Python script (inline), PyCUDA automatically compiles that CUDA code into executable code using the Nvidia CUDA compiler `nvcc`. The compilation only happens the first time the function is called. After the code is compiled, then it is cached and re-used for the remainder of a given Python session.\n",
    "\n",
    "Note: I think the main reason is compilation versus interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We allocate empty memory on the GPU with the `gpuarray.empty_like` function. This acts as a plain `malloc` in C, allocating an array of the same size and data type as `device_data`, but without copying anything. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mandelbroy revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_mandelbrot(width, height, real_low, real_high, imag_low, imag_high, max_iters, upper_bound):\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
